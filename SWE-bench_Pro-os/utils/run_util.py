
import os
import json
import docker
from helper_code.image_uri import get_dockerhub_image_uri


from utils.parser_util import str2bool, get_test_directives, analyze_test_results
from utils.unified_log_parsers import parse_logs_with_unified_parser

def load_base_docker(iid):
    with open(f"dockerfiles/base_dockerfile/{iid}/Dockerfile") as fp:
        return fp.read()


def instance_docker(iid):
    with open(f"dockerfiles/instance_dockerfile/{iid}/Dockerfile") as fp:
        return fp.read()


def load_local_script(scripts_dir, instance_id, script_name):
    """Load a script file from local scripts directory."""
    script_path = os.path.join(scripts_dir, instance_id, script_name)
    if not os.path.exists(script_path):
        raise FileNotFoundError(f"Script not found: {script_path}")

    with open(script_path, 'r') as f:
        return f.read()


def create_entryscript_for_test_patch(sample, use_coverage=False):
    """
    Create entry script that:
    1. Applies the gold patch first
    2. Then applies the model_test_patch (new tests)
    3. Runs the tests
    """
    before_repo_set_cmd = sample["before_repo_set_cmd"].strip().split("\n")[-1]
    selected_test_files_to_run = ",".join(eval(sample["selected_test_files_to_run"]))

    test_directives = get_test_directives(sample, 'model_test_patch')
    repo_language = sample.get("repo_language", "python")
    base_commit = sample["base_commit"]
    base_dockerfile = load_base_docker(sample["instance_id"])
    instance_dockerfile = instance_docker(sample["instance_id"])

    # Extract ENV commands from dockerfiles
    env_cmds = []
    for dockerfile_content in [base_dockerfile, instance_dockerfile]:
        for line in dockerfile_content.split("\n"):
            line = line.strip()
            if line.startswith("ENV"):
                env_cmd = line.replace("ENV", "export", 1)
                env_cmds.append(env_cmd)

    env_cmds = "\n".join(env_cmds)
    # print("repo_language", repo_language)
    # For Go projects, use the extracted test info (package paths + test names)
    if repo_language == "go":
        package_paths = " ".join(test_directives)
        if use_coverage:
            test_command = """COVERAGE_DIR="/workspace/coverage"
mkdir -p "$COVERAGE_DIR"
go test -v -coverprofile="$COVERAGE_DIR/coverage.out" """ + package_paths + """ > /workspace/stdout.log 2> /workspace/stderr.log
"""
        else:
            test_command = f"go test -v {package_paths} > /workspace/stdout.log 2> /workspace/stderr.log"
    else:
        # For other languages, use run_script.sh or run_script_coverage.sh
        if isinstance(test_directives, dict):
            # Shouldn't happen, but handle gracefully
            selected_test_files_to_run = ','.join(test_directives.get("package_paths", []))
        else:
            selected_test_files_to_run = ','.join(test_directives)
        run_script_name = "run_script_coverage.sh" if use_coverage else "run_script.sh"
        test_command = f"bash /workspace/{run_script_name} {selected_test_files_to_run} > /workspace/stdout.log 2> /workspace/stderr.log"

    entry_script = f"""
{env_cmds}
# apply patches
cd /app
git reset --hard {base_commit}
git checkout {base_commit}

# First apply the gold patch (the actual fix)
echo "Applying gold patch..."
git apply -v /workspace/gold_patch.diff
if [ $? -ne 0 ]; then
    echo "ERROR: Failed to apply gold patch"
    exit 1
fi

# Then apply the model test patch (new tests generated by model)
echo "Applying model test patch..."
git apply -v /workspace/model_test_patch.diff
if [ $? -ne 0 ]; then
    echo "ERROR: Failed to apply model test patch"
    exit 1
fi

# run test and save stdout and stderr to separate files
{test_command}

# Fix permissions for files generated in container so host user can read them
chmod -R a+rw /workspace/
"""
    return entry_script


def prepare_run(log_dir, prefix, redo):
    """
        Prepare the runtime environment.

            Args:
                log_dir: Log storage directory (fully assembled path)
                prefix: Output file prefix
                redo: Whether to re-run

            Returns:
                existing_output: Parsed result if output already exists, otherwise None
                output_path: Output file path
                workspace_dir: Working directory path
    """
    os.makedirs(log_dir, exist_ok=True)
    output_path = os.path.join(log_dir, f"{prefix}_output.json")
    if not redo and os.path.exists(output_path):
        print(f"Skipping {log_dir} - output already exists")
        with open(output_path, "r") as f:
            return json.load(f), output_path, os.path.join(log_dir, "workspace")
    workspace_dir = os.path.join(log_dir, "workspace")
    os.makedirs(workspace_dir, exist_ok=True)
    return None, output_path, workspace_dir


def write_patch_snapshot(log_dir, prefix, patch, filename = "patch.diff"):
    """Save the patch to log_dir"""
    with open(os.path.join(log_dir, f"{prefix}_{filename}"), "w") as f:
        f.write(patch)


def assemble_workspace_files_for_test_patch(uid, scripts_dir, gold_patch, model_test_patch, sample, use_coverage=False):
    """
    Prepare workspace files for evaluating model test patch.
    Note: parser.py is no longer needed as we use unified parser on host side.
    """
    # Select script to load based on use_coverage
    run_script_name = "run_script_coverage.sh" if use_coverage else "run_script.sh"
    run_script = load_local_script(scripts_dir, uid, run_script_name)
    entryscript_content = create_entryscript_for_test_patch(sample, use_coverage=use_coverage)

    files = {
        "gold_patch.diff": gold_patch,
        "model_test_patch.diff": model_test_patch,
        run_script_name: run_script,
        "entryscript.sh": entryscript_content,
    }
    return files, entryscript_content


def write_files_local(workspace_dir, files):
    for rel_path, content in files.items():
        dst = os.path.join(workspace_dir, rel_path)
        with open(dst, "w") as f:
            f.write(content)


def save_entryscript_copy(log_dir, prefix, entryscript_content):
    """Save the entryscript to log_dir"""
    with open(os.path.join(log_dir, f"{prefix}_entryscript.sh"), "w") as f:
        f.write(entryscript_content if entryscript_content is not None else "")

def collect_outputs_local(workspace_dir, log_dir, repo_name, uid, prefix):
    """
        Collect log files output by the container and parse them using the unified parser.

            Args:
                workspace_dir: Working directory
                log_dir: Log storage directory (fully assembled path)
                repo_name: Repository name
                uid: instance_id (used for log parsing only, not for path construction)
                prefix: Output file prefix

            Returns:
                dict: Parsed output, compatible with the original output.json format
    """
    stdout_content = ""
    stderr_content = ""

    # Read stdout.log
    stdout_src = os.path.join(workspace_dir, "stdout.log")
    stdout_dest = os.path.join(log_dir, f"{prefix}_stdout.log")
    try:
        with open(stdout_src, "r", errors='replace') as f_in:
            stdout_content = f_in.read()
    except FileNotFoundError:
        stdout_content = ""
    with open(stdout_dest, "w") as f_out:
        f_out.write(stdout_content)

    # Read stderr.log
    stderr_src = os.path.join(workspace_dir, "stderr.log")
    stderr_dest = os.path.join(log_dir, f"{prefix}_stderr.log")
    try:
        with open(stderr_src, "r", errors='replace') as f_in:
            stderr_content = f_in.read()
    except FileNotFoundError:
        stderr_content = ""
    with open(stderr_dest, "w") as f_out:
        f_out.write(stderr_content)

    # Parse logs using the unified parser
    if not stdout_content and not stderr_content:
        print(f"Warning: No stdout.log or stderr.log for {uid}")
        return None

    output = parse_logs_with_unified_parser(repo_name, uid, stdout_content, stderr_content)

    # Check if parsing encountered errors
    if "error" in output:
        print(f"Warning: Parse error for {uid}: {output['error']}")
        return None

    # Save parsing results
    with open(os.path.join(log_dir, f"{prefix}_output.json"), "w") as f:
        json.dump(output, f, indent=4, ensure_ascii=False)

    return output



def run_docker(args, uid, workspace_dir, log_dir, scripts_dir, patch, model_test_patch, sample, prefix, result, repo_name,global_logger,
               dockerhub_username='jefzda', docker_platform=None, mem_limit=None, timeout=None, block_network=False,use_coverage=False):
    """
        Run a Docker container to execute tests.

            Args:
                args: Command-line arguments
                uid: instance_id
                workspace_dir: Working directory
                log_dir: Log storage directory (fully assembled path)
                scripts_dir: Scripts directory
                patch: gold patch or agent patch
                model_test_patch: Model-generated test patch
                sample: Sample data
                prefix: Output file prefix
                result: Result dictionary
                repo_name: Repository name
                dockerhub_username: Docker Hub username
                docker_platform: Docker platform
                mem_limit: Memory limit
                timeout: Timeout duration
                block_network: Whether to block network access
    """
    try:
        files, entryscript_content = assemble_workspace_files_for_test_patch(
            uid, scripts_dir, patch, model_test_patch, sample, use_coverage=use_coverage
        )
    except FileNotFoundError as e:
        result["error"] = f"Error loading scripts: {e}"
        global_logger.error(f"Error loading scripts for {uid}: {e}")
        return result

    # Write files to workspace
    write_files_local(workspace_dir, files)

    # Save patches for debugging
    write_patch_snapshot(log_dir, prefix, patch, "gold_patch.diff")
    write_patch_snapshot(log_dir, prefix, model_test_patch, "model_test_patch.diff")

    # Get Docker image
    dockerhub_image_uri = get_dockerhub_image_uri(uid, dockerhub_username, sample.get("repo", ""))
    # print(f"Using Docker Hub image: {dockerhub_image_uri}")

    client = docker.from_env()
    try:
        if docker_platform:
            client.images.pull(dockerhub_image_uri, platform=docker_platform)
        else:
            client.images.pull(dockerhub_image_uri)
    except Exception as pull_err:
        try:
            client.images.get(dockerhub_image_uri)
            global_logger.error(f"Using locally available image: {dockerhub_image_uri}")
        except Exception:
            result["error"] = f"Failed to pull or find image: {pull_err}"
            global_logger.error(f"Failed to pull or find image locally for {uid}: {pull_err}")
            return result

    abs_workspace_dir = os.path.abspath(workspace_dir)
    volumes = {abs_workspace_dir: {"bind": "/workspace", "mode": "rw"}}
    run_kwargs = {
        "volumes": volumes,
        "detach": True,
        "remove": False,
        "entrypoint": "/bin/bash",
        "command": ["-c", "bash /workspace/entryscript.sh"],
    }
    if block_network:
        run_kwargs["network_mode"] = "none"
    if docker_platform:
        run_kwargs["platform"] = docker_platform
    if mem_limit:
        run_kwargs["mem_limit"] = mem_limit
        run_kwargs['memswap_limit'] = mem_limit

    container = client.containers.run(
        dockerhub_image_uri,
        **run_kwargs,
    )

    timed_out = False
    try:
        container_result = container.wait(timeout=timeout)
        status_code = container_result.get("StatusCode", 1) if isinstance(container_result, dict) else 1
    except Exception as wait_err:
        # Timeout or other wait error
        timed_out = True
        status_code = -1
        global_logger.error(f"Container timeout or wait error for {uid}: {wait_err}")
        try:
            container.remove()
        except Exception:
            pass

    # Save raw container logs (stdout + stderr from Docker)
    try:
        raw_logs = container.logs(stdout=True, stderr=True).decode("utf-8", errors="replace")
        raw_log_path = os.path.join(log_dir, "raw_container.log")
        with open(raw_log_path, "w") as f:
            f.write(raw_logs)
    except Exception as log_err:
        global_logger.error(f"Failed to save raw container logs for {uid}: {log_err}")


    if status_code != 0 and not timed_out:
        global_logger.error(f"Entryscript failed for {uid} with return code: {status_code}")

    # Collect outputs
    output = collect_outputs_local(workspace_dir, log_dir, repo_name, uid, prefix)
    save_entryscript_copy(log_dir, prefix, entryscript_content)

    # Clean up container
    try:
        container.remove()
    except Exception:
        pass

    return output, timed_out

